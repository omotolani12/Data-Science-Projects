{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is about developing a supervised learning model on a genomic dataset. The dataset contains data about patients admitted in hospitals to treat COVID-19 cases. Some patients had a mild infection, while others had a severe one. The goal of this project is to build a supervised learning model to predict severe COVID-19 hospital admissions by using the genome sequencing of patients as predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing\n",
    "\n",
    "\n",
    "\n",
    "1. Importing of some of the libraries needed for the project\n",
    "2. Data was collected and imported into this environment using pandas.\n",
    "3. The column \"sample\" was dropped. It is not needed for this project because it doesnt improve the outcome and also takes away any form of personalization\n",
    "4. Sex and Severity data was converted to binary form for data uniformity(All numerical data)\n",
    "5. The dataset was printed out to view the result after some data processing has been done\n",
    "6. Data was checked for missing values so columns with a level of missing values can be dropped to reduce dimensionality.\n",
    "7. X and Y was defined for the training and test process\n",
    "8.Structure of X was viewed and examined ensure it meets my required standard\n",
    "9.Structure of Y was viewed and examined to ensure it meets my required standard\n",
    "10.The dataset X was printed out to view the result after some data processing has been done\n",
    "11.The dataset Y was printed out to view the result after some data processing has been done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #imports the library numpy\n",
    "import matplotlib.pyplot as plt #imports the library matplotlib for visualization\n",
    "import pandas as pd #imports the library numpy for working with dataframes\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Severity</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A3GALT2</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>...</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1</td>\n",
       "      <td>39</td>\n",
       "      <td>male</td>\n",
       "      <td>NonICU</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.84</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.63</td>\n",
       "      <td>15.51</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.17</td>\n",
       "      <td>363.01</td>\n",
       "      <td>19.17</td>\n",
       "      <td>6.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C2</td>\n",
       "      <td>63</td>\n",
       "      <td>male</td>\n",
       "      <td>NonICU</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.55</td>\n",
       "      <td>12.15</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>15.62</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.20</td>\n",
       "      <td>399.80</td>\n",
       "      <td>15.72</td>\n",
       "      <td>4.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C3</td>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>NonICU</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.32</td>\n",
       "      <td>17.67</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.62</td>\n",
       "      <td>430.35</td>\n",
       "      <td>13.95</td>\n",
       "      <td>1.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C4</td>\n",
       "      <td>49</td>\n",
       "      <td>male</td>\n",
       "      <td>NonICU</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.71</td>\n",
       "      <td>5.87</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.21</td>\n",
       "      <td>15.61</td>\n",
       "      <td>0.27</td>\n",
       "      <td>7.88</td>\n",
       "      <td>209.25</td>\n",
       "      <td>14.78</td>\n",
       "      <td>7.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C5</td>\n",
       "      <td>49</td>\n",
       "      <td>male</td>\n",
       "      <td>NonICU</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.34</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.96</td>\n",
       "      <td>272.91</td>\n",
       "      <td>8.69</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 19476 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sample  Age   Sex Severity  A1BG  A1CF   A2M  A2ML1  A3GALT2  A4GALT  ...  \\\n",
       "0     C1   39  male   NonICU  0.49  0.00  0.21   0.04     0.07     0.0  ...   \n",
       "1     C2   63  male   NonICU  0.29  0.00  0.14   0.00     0.00     0.0  ...   \n",
       "2     C3   33  male   NonICU  0.26  0.00  0.03   0.02     0.00     0.0  ...   \n",
       "3     C4   49  male   NonICU  0.45  0.01  0.09   0.07     0.00     0.0  ...   \n",
       "4     C5   49  male   NonICU  0.17  0.00  0.00   0.05     0.07     0.0  ...   \n",
       "\n",
       "   ZWILCH  ZWINT  ZXDA  ZXDB   ZXDC  ZYG11A  ZYG11B     ZYX  ZZEF1  ZZZ3  \n",
       "0    2.84   4.22  0.95  1.63  15.51    0.06    8.17  363.01  19.17  6.05  \n",
       "1    3.55  12.15  0.60  1.15  15.62    0.14    8.20  399.80  15.72  4.12  \n",
       "2    1.34   2.79  0.18  0.32  17.67    0.28    3.62  430.35  13.95  1.81  \n",
       "3    3.71   5.87  1.40  2.21  15.61    0.27    7.88  209.25  14.78  7.15  \n",
       "4    1.44   4.46  0.28  0.55   9.34    0.07    5.96  272.91   8.69  2.70  \n",
       "\n",
       "[5 rows x 19476 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we are loading our dataset as an csv file\n",
    "covid_data = pd.read_csv(\"covid_data.csv\")\n",
    "covid_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The column \"sample\" was dropped. It is not needed for this project because it doesnt improve the outcome and also takes away any form of personalization\n",
    "covid_data.drop('Sample', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data['Severity'] = covid_data['Severity'].map({'NonICU':1,'ICU':0}) #Changing objects to binary\n",
    "covid_data['Sex'] = covid_data['Sex'].map({'male':0,'female':1,'unknown':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Severity</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A3GALT2</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>A4GNT</th>\n",
       "      <th>...</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.84</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.63</td>\n",
       "      <td>15.51</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.17</td>\n",
       "      <td>363.01</td>\n",
       "      <td>19.17</td>\n",
       "      <td>6.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>3.55</td>\n",
       "      <td>12.15</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>15.62</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.20</td>\n",
       "      <td>399.80</td>\n",
       "      <td>15.72</td>\n",
       "      <td>4.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.32</td>\n",
       "      <td>17.67</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.62</td>\n",
       "      <td>430.35</td>\n",
       "      <td>13.95</td>\n",
       "      <td>1.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.71</td>\n",
       "      <td>5.87</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.21</td>\n",
       "      <td>15.61</td>\n",
       "      <td>0.27</td>\n",
       "      <td>7.88</td>\n",
       "      <td>209.25</td>\n",
       "      <td>14.78</td>\n",
       "      <td>7.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.34</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.96</td>\n",
       "      <td>272.91</td>\n",
       "      <td>8.69</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 19475 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Sex  Severity  A1BG  A1CF   A2M  A2ML1  A3GALT2  A4GALT  A4GNT  ...  \\\n",
       "0   39    0         1  0.49  0.00  0.21   0.04     0.07     0.0   0.03  ...   \n",
       "1   63    0         1  0.29  0.00  0.14   0.00     0.00     0.0   0.05  ...   \n",
       "2   33    0         1  0.26  0.00  0.03   0.02     0.00     0.0   0.07  ...   \n",
       "3   49    0         1  0.45  0.01  0.09   0.07     0.00     0.0   0.00  ...   \n",
       "4   49    0         1  0.17  0.00  0.00   0.05     0.07     0.0   0.00  ...   \n",
       "\n",
       "   ZWILCH  ZWINT  ZXDA  ZXDB   ZXDC  ZYG11A  ZYG11B     ZYX  ZZEF1  ZZZ3  \n",
       "0    2.84   4.22  0.95  1.63  15.51    0.06    8.17  363.01  19.17  6.05  \n",
       "1    3.55  12.15  0.60  1.15  15.62    0.14    8.20  399.80  15.72  4.12  \n",
       "2    1.34   2.79  0.18  0.32  17.67    0.28    3.62  430.35  13.95  1.81  \n",
       "3    3.71   5.87  1.40  2.21  15.61    0.27    7.88  209.25  14.78  7.15  \n",
       "4    1.44   4.46  0.28  0.55   9.34    0.07    5.96  272.91   8.69  2.70  \n",
       "\n",
       "[5 rows x 19475 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It is a high dimensional data since the number of features is greater than the number of observations\n",
    "covid_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This was done to see if the dataset has missing values and to also drop columns with the most missing values to help reduce dimensionality but we have zero missing values in the dataset\n",
    "covid_data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block separates features (X) from outcomes (y)\n",
    "X = covid_data.drop('Severity', axis=1) \n",
    "y = covid_data['Severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 19474)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape #To view the data structure of X\n",
    "#It is a high dimensional data since the number of features is greater than the number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape #To view the data structure of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A3GALT2</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>A4GNT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>...</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>18.92</td>\n",
       "      <td>...</td>\n",
       "      <td>2.84</td>\n",
       "      <td>4.22</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.63</td>\n",
       "      <td>15.51</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.17</td>\n",
       "      <td>363.01</td>\n",
       "      <td>19.17</td>\n",
       "      <td>6.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>18.68</td>\n",
       "      <td>...</td>\n",
       "      <td>3.55</td>\n",
       "      <td>12.15</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>15.62</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.20</td>\n",
       "      <td>399.80</td>\n",
       "      <td>15.72</td>\n",
       "      <td>4.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>13.85</td>\n",
       "      <td>...</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.32</td>\n",
       "      <td>17.67</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.62</td>\n",
       "      <td>430.35</td>\n",
       "      <td>13.95</td>\n",
       "      <td>1.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.11</td>\n",
       "      <td>...</td>\n",
       "      <td>3.71</td>\n",
       "      <td>5.87</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.21</td>\n",
       "      <td>15.61</td>\n",
       "      <td>0.27</td>\n",
       "      <td>7.88</td>\n",
       "      <td>209.25</td>\n",
       "      <td>14.78</td>\n",
       "      <td>7.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.45</td>\n",
       "      <td>...</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.34</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.96</td>\n",
       "      <td>272.91</td>\n",
       "      <td>8.69</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 19474 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Sex  A1BG  A1CF   A2M  A2ML1  A3GALT2  A4GALT  A4GNT   AAAS  ...  \\\n",
       "0   39    0  0.49  0.00  0.21   0.04     0.07     0.0   0.03  18.92  ...   \n",
       "1   63    0  0.29  0.00  0.14   0.00     0.00     0.0   0.05  18.68  ...   \n",
       "2   33    0  0.26  0.00  0.03   0.02     0.00     0.0   0.07  13.85  ...   \n",
       "3   49    0  0.45  0.01  0.09   0.07     0.00     0.0   0.00  22.11  ...   \n",
       "4   49    0  0.17  0.00  0.00   0.05     0.07     0.0   0.00   8.45  ...   \n",
       "\n",
       "   ZWILCH  ZWINT  ZXDA  ZXDB   ZXDC  ZYG11A  ZYG11B     ZYX  ZZEF1  ZZZ3  \n",
       "0    2.84   4.22  0.95  1.63  15.51    0.06    8.17  363.01  19.17  6.05  \n",
       "1    3.55  12.15  0.60  1.15  15.62    0.14    8.20  399.80  15.72  4.12  \n",
       "2    1.34   2.79  0.18  0.32  17.67    0.28    3.62  430.35  13.95  1.81  \n",
       "3    3.71   5.87  1.40  2.21  15.61    0.27    7.88  209.25  14.78  7.15  \n",
       "4    1.44   4.46  0.28  0.55   9.34    0.07    5.96  272.91   8.69  2.70  \n",
       "\n",
       "[5 rows x 19474 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head() #To view the first five data in X and confirm it is in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: Severity, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head() #To view the first five data in X and confirm it is in the required f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training and tuning\n",
    "\n",
    "Please note that PCA and reglarization was not included to reduce dimensionality because it didn't change the performance of the model.\n",
    "\n",
    "12. The block below splits our dataset into training(80%) and testing(20%). We keep test data unseen so we can use later on to measure the performance of our model.\n",
    "\n",
    "15. The shape of the X_train and y_test data was printed to see the splitting result\n",
    "\n",
    "16.  The value count of Y train and test was checked to ensure there was a proper or close to eaqual split for the model to learn from.\n",
    "\n",
    "17. Standardization was done for X for because the features of the iput data X have difference beteween their range(some columns have values as low as 0.1 and others as high as 430.35). Standardization was used to transform features to comparable scales before implementing SVM\n",
    "\n",
    "18. SVM was one models used first to compare accuracries accross models before picking the best fit for the data. I decided to try linear SVM kernel because the data had a large number of features and it is linearly separable in high dimensional space.\n",
    "\n",
    "19. Accuracy of all SVM kernel of the test data set was printed out for evaluation. I did not show to prediction reult and confusion matrix for this model because i ended up selecting logistic regression.\n",
    "\n",
    "20. Creating tuning and validation data for k-cross validation before using Logistic regression. This was done to ensure that every observation in the dataset has chance to appearing because performs training 'k'number of time which gives a better insight.Tuning data is composed by 80% of data, the remaining 20% of data goes to the validation set.The variablerandom_state indicates the seed of the shuffling (for reproducibility reasons).\n",
    "\n",
    "23. PIPELINE FOR IMPLEMENTING LOGISTIC REGRESSION\n",
    "\n",
    "Standardisation of features:- Standardization was used to transform features to comparable scales before implementing logistic regression\n",
    "Logistic regression was used since we were solving a binary classification problem.\n",
    "Random Forest was also explored to further see the best model because it is a model good for datasets with numerous features\n",
    "As for the logistic regression, we are using the LogisticRegression module from sklearn.linear_model. \n",
    "\n",
    "Some important parameters defined in the LogisticRegression module are:\n",
    "\n",
    "C. Inverse of regularization strength; must be a positive float. Smaller values was used to specify stronger regularization.\n",
    "Random_state was used reproducibility reasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block below splits our dataset into training(80%) and testing(20%). We keep test data unseen, so to later on measure the performance of our model.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 19474)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape #Features on training data contain 100 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape #Features on test data contain 26 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    51\n",
       "0    49\n",
       "Name: Severity, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts() #Feature on y train contains 51 ICU cases and 49 non-icu cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17\n",
       "1     9\n",
       "Name: Severity, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()#Feature on y train contains 17 ICU cases and 9 non-icu cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardising  data\n",
    "#Standardization was used to transform features to comparable scales before implementing SVM\n",
    "from sklearn.preprocessing import StandardScaler # Library to standardise data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train) \n",
    "X_test = scaler.transform(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create many svm Classifier, one classifier for each kernel\n",
    "clf_lnr = svm.SVC(kernel='linear') # Linear Kernel\n",
    "clf_ply = svm.SVC(kernel='poly') # Polynomial Kernel\n",
    "clf_rbf = svm.SVC(kernel='rbf') # Radial Basis Function Kernel - Default value\n",
    "clf_sgm = svm.SVC(kernel='sigmoid') # Sigmoid Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf_lnr.fit(X_train, y_train)\n",
    "clf_ply.fit(X_train, y_train)\n",
    "clf_rbf.fit(X_train, y_train)\n",
    "clf_sgm.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "pred_test_lnr = clf_lnr.predict(X_test)\n",
    "pred_test_ply = clf_ply.predict(X_test)\n",
    "pred_test_rbf = clf_rbf.predict(X_test)\n",
    "pred_test_sgm = clf_sgm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy SVM (Linear Kernel): 0.8461538461538461\n",
      "Accuracy SVM (Polynomial Kernel): 0.8076923076923077\n",
      "Accuracy SVM (Radial Kernel): 0.8076923076923077\n",
      "Accuracy SVM (Sigmoid Kernel): 0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "#The accuracy on the linear kernel had the best result because our data is linearly separable.\n",
    "print(\"Accuracy SVM (Linear Kernel):\", metrics.accuracy_score(y_test, pred_test_lnr)) \n",
    "print(\"Accuracy SVM (Polynomial Kernel):\", metrics.accuracy_score(y_test, pred_test_ply))\n",
    "print(\"Accuracy SVM (Radial Kernel):\", metrics.accuracy_score(y_test, pred_test_rbf))\n",
    "print(\"Accuracy SVM (Sigmoid Kernel):\", metrics.accuracy_score(y_test, pred_test_sgm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tuning and validation data for k-cross validation before using Logistic regression\n",
    "#Splits our dataset into tuning and validation. We keep validation data unseen, so to later on measure the performance of our model.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tuning, X_val, y_tuning, y_val = train_test_split(X, y, test_size=0.20, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This was done to ensure that every observation in the dataset has chance to appearing because it performs training 'k-1'number of time which gives a better insight\n",
    "#K was used as 10 to make it sufficiently reliable to test my model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 10, shuffle=True, random_state=10)\n",
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each alpha, perform k-fold cross validation\n",
    "for train_index, test_index in kf.split(X_tuning):\n",
    "        X_train, X_test = X_tuning.iloc[train_index], X_tuning.iloc[test_index]\n",
    "        y_train, y_test = y_tuning.iloc[train_index], y_tuning.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 2.262s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler # Library to standardise data\n",
    "from sklearn.linear_model import LogisticRegression # Library to use Logistic Regression\n",
    "from sklearn.ensemble import RandomForestClassifier # Library to use Random Forest\n",
    "from sklearn.pipeline import Pipeline # Library to create a pipeline\n",
    "from time import time \n",
    "\n",
    "start = time()\n",
    "\n",
    "# Defining different pipelines\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), # scaling features\n",
    "                     ('logreg', LogisticRegression(C=1, random_state=5))]) # logistic regression\n",
    "rndf_clf = RandomForestClassifier(oob_score=True, random_state=10) # no need to scale features for random forest\n",
    "\n",
    "# Implementing our pipelines\n",
    "pipe.fit(X_train, y_train)\n",
    "rndf_clf.fit(X, y) # no need to use train-test in Random Forest, we are using the oob_score\n",
    "\n",
    "\n",
    "\n",
    "print(\"This took %0.3fs\" % (time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model validation\n",
    "\n",
    "\n",
    "\n",
    "28. I decided to use  logistic regression model because it gives a good confusion matrix and classification result compared to random forest.From the results it can be observed that there is only one misclassification for ICU cases and 1 misclassification for NonICU cases\n",
    "\n",
    "29. Precision is intuitively the ability of the classifier not to label as positive a sample that is negative and Logistic regression performed well for both NonICU(1) and ICU(0) cases\n",
    "The recall is intuitively the ability of the classifier to find all the positive samples Logistic regression also performed well for both NonICU(1) and ICU(0) cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic - Accuracy on train: 1.0\n",
      "Logistic - Accuracy on test: 0.8\n",
      " - \n",
      "Random Forest - Accuracy on train: 1.0\n",
      "Random Forest - Accuracy on test: 0.8015873015873016\n"
     ]
    }
   ],
   "source": [
    "#The accuracy of Logistic regrassion and random forest is printed out in this block\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Logistic - Accuracy on train:\", accuracy_score(lreg_train.actual, lreg_train.fitted))\n",
    "print(\"Logistic - Accuracy on test:\", accuracy_score(lreg_test.actual, lreg_test.fitted))\n",
    "\n",
    "print(\" - \")\n",
    "print(\"Random Forest - Accuracy on train:\", accuracy_score(rndf_train.actual, rndf_train.fitted))\n",
    "print(\"Random Forest - Accuracy on test:\", rndf_clf.oob_score_) # the oob_score from random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1]\n",
      " [1 5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75         4\n",
      "           1       0.83      0.83      0.83         6\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.79      0.79      0.79        10\n",
      "weighted avg       0.80      0.80      0.80        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#From the results it can be observed that there is only one misclassification for ICU cases and 1 misclassification for NonICU cases\n",
    "#Precision is intuitively the ability of the classifier not to label as positive a sample that is negative and Logistic regression performed well for both NonICU(1) and ICU(0) cases\n",
    "#The recall is intuitively the ability of the classifier to find all the positive samples Logistic regression also performed well for both NonICU(1) and ICU(0) cases\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(lreg_test.actual, lreg_test.fitted)\n",
    "print(cm)\n",
    "print(classification_report(lreg_test.actual, lreg_test.fitted))\n",
    "#This block below computes the precision, recall, f-score, macro and weighted average for each class of test data where 1 = Nonicu and 0 = ICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5238095238095238"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Our accuracy is 0.80, which is a big jump from the value we would get from a dummy classifier.\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X, y)\n",
    "DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.score(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00609459, 0.00212979, 0.00036623, ..., 0.00337328, 0.00237115,\n",
       "        0.00011048]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipeline used was for standardization and logistic regression\n",
    "pipe[1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model interpretation\n",
    "\n",
    "\n",
    "The regression coefficient tells us if there is a positive or negative correlation between the independent variable and the dependent varable. \n",
    "The barplot below means that a negative change of ICU correspond to an increase of the probability of predicting a patients case as none 'Non-ICU' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value0</th>\n",
       "      <th>beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006095</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002130</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000366</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000150</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     value0  beta\n",
       "0  0.006095   1.0\n",
       "1  0.002130   1.0\n",
       "2  0.000366   1.0\n",
       "3 -0.001001   1.0\n",
       "4 -0.000150   1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression works only on binomial variables.\n",
    "#The result below shows the result for each observation\n",
    "#The barplot below futher expains the interpretation\n",
    "# ICU(1)  VS NONICU(0)\n",
    "coef = pd.DataFrame({\"value0\": pipe[1].coef_[0]})\n",
    "coef[\"beta\"] = y\n",
    "\n",
    "coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26aec048ca0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The sign of a regression coefficient tells you whether there is a positive or negative correlation between each independent variable and the dependent variable\n",
    "#A negative coefficient suggests that as the independent variable(ICU) decreases, the dependent variable tends to increases(NON-ICU).\n",
    "#The barplot below means that a negative change of ICU correspond to an increase of the probability of predicting a patients case as none 'Non-ICU'\n",
    "import seaborn as sb\n",
    "sb.set(rc={'figure.figsize':(10,5)})\n",
    "sb.barplot(data = coef, x=\"beta\", y=\"value0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26aec13a100>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAE1CAYAAADaj4uUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAar0lEQVR4nO3df3BV5Z3H8c/FhAC2d7uuN0TDqsXaNZvGJvJD0m4THZYfQ2hJ0gxaqEmhzZKMSdp0JsCGCJXir8JC1bvsMo6yQSM2FBLsVkgd7HR0A0Uzs0AkMopLS24wP7R4TSAm4Z79o/bWcMO94Uru4z33/WLODOece57ny19+/T7f8xyHZVmWAAAADBhnOgAAABC7SEQAAIAxJCIAAMAYEhEAAGAMiQgAADCGRAQAABgTZzoAAAAQPe699169//77iov7cwqxfv16ffWrX/Xfb2tr05o1a9TX16fp06frgQce8P92JA72EQEAAKNhWZaysrL029/+9pLJxcKFC7Vhwwalp6erurpaX/nKV7RkyZJLjsnSDAAAGJV33nlHkrR8+XJ961vf0rPPPjvsvsfjUX9/v9LT0yVJ+fn52r9/f9AxWZoBACDGeb1eeb3egOtOp1NOp3PY7zIzM3X//fdrcHBQhYWF+uIXv6ivf/3rkqSuri65XC7/710ulzo7O4POHdFE5PUpuZGcDoCkWV2vmQ4BiFlDA56IzjfY805Yz9Xu/LXcbnfA9bKyMpWXl/vPMzIylJGR4T8vKCjQ7373O38i4vP55HA4/Pctyxp2PhIqIgAA2IXvQliPFRUVKS8vL+D6J6shkvT6669rcHBQmZmZkv6caHyyVyQpKUnd3d3+856eHiUmJgadmx4RAADswvKFdTidTk2ZMiXguDgR+fDDD/Wzn/1MH330kXp7e9XQ0KA5c+b47ycnJyshIUEtLS2SpL179yorKytoyFREAACwC59vTIe/6667dOTIEeXm5srn82nJkiXKyMhQcXGxKioqlJaWpk2bNqmmpka9vb1KTU1VYWFh0DEj+vouPSJA5NEjApgT6R6RgY43wnpu/PWpVziS0aMiAgCAXYxxRWQskIgAAGAXFokIAAAwJcy3ZkwiEQEAwC6isCLC67sAAMAYKiIAANgFzaoAAMAUKwqXZkhEAACwCyoiAADAGCoiAADAGF7fBQAAxlARAQAAxtAjAgAAjKEiAgAAjKEiAgAATLEsmlUBAIApLM0AAABjWJoBAADGUBEBAADGsKEZAAAwhooIAAAwJgp7RMaZDgAAAMQuKiIAANgFSzMAAMCYKFyaIREBAMAuSEQAAIApkdri/dFHH9Wf/vQnPfLII8Ouu91u7d69W06nU5K0ePFiLV26NOhYJCIAANhFBCoiBw8eVENDg+68886Ae62trdq8ebMyMjJGPR6JCAAAdhFms6rX65XX6w247nQ6/dUNSTp79qy2bNmikpISvfnmmwG/b21t1bZt2+TxeDRjxgytWrVKCQkJQefm9V0AAOzC5wvrqK2t1ezZswOO2traYcOvXbtWlZWVw5KTv+jr61NKSoqqqqrU0NAgr9errVu3hgyZiggAAHYRZkWkqKhIeXl5Adc/mXDs2rVL1113nTIzM7Vnz56A31599dV68skn/efLly9XdXW1Kisrg85NIgIAgF2E2SNy8RLMSF588UV1d3dr0aJF+uCDD3Tu3Dk99NBDqq6uliR1dHSoublZBQUFkiTLshQXFzrNIBEBAMAuxnBDs+3bt/v/vmfPHh0+fNifhEjShAkTtHHjRt1xxx2aMmWK6urqNGfOnJDj0iMCAIBdhNkj8mkUFxfr2LFjuuaaa7R+/XqVlpZq/vz5sixLy5YtC/m8w7Is61NFcBlen5IbqakAfGxW12umQwBi1tCAJ6Lznf/1z8N6bmLOj65wJKPH0gwAAHbBt2YAAIAxbPEOAACMoSICAACMicKKCG/NAAAAY6iIAABgFyzNAAAAY6JwaYZEBAAAuyARAQAAxkRuj9IrhkQEAAC7oCICAACMIREBAADG8NYMAAAwhooIAAAwhmZVAABgDBURAABgDIkIAAAwhmZVAABgiuWjRwQAAJjC0gwAADCGpRkAAGBMFC7NjDMdAAAAiF1URAAAsAt6RAAAgDEkIrAb1/cWKPHe+ZJlqf8P7+oPK7dq6L0PTIcFxIynn/q5WlvbtHnLNtOhIBpE4Rbv9Ijgkial3aykFbl6M3e13vjnH+qj/zuj5KolpsMCYsKtt35JLzXV69v5OaZDQTTx+cI7LtOjjz6q1atXB1xva2tTfn6+5s2bpzVr1mhoaCjkWCErIidPnlRTU5PeffddjRs3TomJifrGN76htLS0yw4c0eXcsZNq/UaprKELciTEKz7pGg2c7jIdFhATSku+p6e2P6c/nvaYDgXRJAJvzRw8eFANDQ268847A+5VVVVpw4YNSk9PV3V1terr67VkSfD/gQ1aEamrq9OPf/xjSVJaWppSU1MlSffff7+efvrpMP8JiCbW0AV9Yd4duu21p/T5WanqqT9gOiQgJvzwRzV6/vlG02Eg2li+sA6v16v29vaAw+v1Dhv+7Nmz2rJli0pKSgKm9ng86u/vV3p6uiQpPz9f+/fvDxly0IrIjh071NjYqIkTJw67vmzZMuXl5Wn58uUhJ0D0O9v0e51t+r2uXTJHX352nY79U2lUrkMCgO2FWRGpra2V2+0OuF5WVqby8nL/+dq1a1VZWakzZ84E/Larq0sul8t/7nK51NnZGXLuoIlIXFzciOs7/f39io+PDzk4olvCTUmKd/2tel9rkyT1PH9ANz5coqv+5nO6cPZDw9EBAC5mhfnWTFFRkfLy8gKuO51O/9937dql6667TpmZmdqzZ0/Ab30+nxwOx19jsaxh55cSNBEpKSlRbm6uMjMz5XK55HA41NXVpUOHDqmysjLk4Ihu8YnXaOq//1jH51Zq6E8f6u/ysnT+xB9JQgDgsyrMiojT6RyWdIzkxRdfVHd3txYtWqQPPvhA586d00MPPaTq6mpJUlJSkrq7u/2/7+npUWJiYsi5gyYi3/zmNzVz5kwdPHhQXV1d8vl8mj59usrLyzV58uTR/NsQxXoPH9eZx3+pf9i1QdYFnwY739fb33/YdFgAgEsZw2/NbN++3f/3PXv26PDhw/4kRJKSk5OVkJCglpYWTZs2TXv37lVWVlbIcUO+NTN58mTl5uaGGTaiXfcz+9X9TOhmIwBj4/s/oPqMy2DgWzPFxcWqqKhQWlqaNm3apJqaGvX29io1NVWFhYUhn3dYVuS6Dl+fQkIDRNqsrtdMhwDErKGByL5+3feT74T13NU/2XmFIxk9dlYFAMAuovDruyQiAADYxRj2iIwVEhEAAOyCiggAADAl3H1ETOKjdwAAwBgqIgAA2AVLMwAAwBgSEQAAYAxvzQAAAGOoiAAAAFMsEhEAAGAMiQgAADAmCvcRIREBAMAuqIgAAABjSEQAAIAplkUiAgAATKEiAgAAjCERAQAAprCPCAAAMIdEBAAAGBN924iQiAAAYBcszQAAAHOiMBEZZzoAAAAQu6iIAABgF/SIAAAAU+gRAQAA5kSgIvLYY4+pqalJDodDBQUFWrZs2bD7brdbu3fvltPplCQtXrxYS5cuveR4JCIAANjEWFdEDh8+rEOHDumFF17Q0NCQFixYoOzsbE2dOtX/m9bWVm3evFkZGRmjGpNEBAAAuwizIuL1euX1egOuO51Of2VDkmbOnKkdO3YoLi5OnZ2dunDhgiZNmjTsmdbWVm3btk0ej0czZszQqlWrlJCQcMm5eWsGAACbsHzhHbW1tZo9e3bAUVtbGzBHfHy8Hn/8ceXk5CgzM1OTJ0/23+vr61NKSoqqqqrU0NAgr9errVu3Bo3ZYUXwm8GvT8mN1FQAPjar6zXTIQAxa2jAE9H53svJDuu5+J2/GlVF5JPOnz+vkpISLViwQHffffeIvzl+/Liqq6vV2Nh4yblZmgEAwCasMJdmgiUcn3Ty5EkNDAwoJSVFEydO1Ny5c3XixAn//Y6ODjU3N6ugoODP8ViW4uKCpxoszQAAYBe+MI9Ram9vV01NjQYGBjQwMKADBw5o2rRp/vsTJkzQxo0bdfr0aVmWpbq6Os2ZMyfomFREAACwiXArIqOVnZ2to0ePKjc3V1dddZXmzp2rnJwcFRcXq6KiQmlpaVq/fr1KS0s1ODio22+/PeD13ovRIwLYHD0igDmR7hHpmh1ej0jigd9d4UhGj4oIAAA2MdYVkbFAIgIAgF1YDtMRXDYSEQAAbIKKCAAAMMbyUREBAACGRGNFhH1EAACAMVREAACwCYtmVQAAYEo0Ls2QiAAAYBM0qwIAAGMit1f6lUMiAgCATVARAQAAxpCIAAAAY1iaAQAAxlARAQAAxrCPCAAAMIZ9RAAAgDE+KiIAAMAUlmYAAIAxNKsCAABjeH0XAAAYQ0UEAAAYE43NquNMBwAAAGIXFREAAGyCt2YAAIAx0disytIMAAA24bMcYR2X47HHHtOCBQuUk5Oj7du3B9xva2tTfn6+5s2bpzVr1mhoaCjoeCQiAADYhGU5wjpG6/Dhwzp06JBeeOEF7d69W88884zeeeedYb+pqqrS2rVr1dTUJMuyVF9fH3RMEhEAAGzCssI7vF6v2tvbAw6v1zts/JkzZ2rHjh2Ki4vTe++9pwsXLmjSpEn++x6PR/39/UpPT5ck5efna//+/UFjjmiPyKyu1yI5HQBJ5zteMR0CgAgJ9/Xd2tpaud3ugOtlZWUqLy8fdi0+Pl6PP/64nn76ac2fP1+TJ0/23+vq6pLL5fKfu1wudXZ2Bp2bZlUAAGwi3LdmioqKlJeXF3Dd6XSO+PuKigoVFxerpKRE9fX1uvvuuyVJPp9PDsdfY7Asa9j5SEhEAACwiXArIk6n85JJxyedPHlSAwMDSklJ0cSJEzV37lydOHHCfz8pKUnd3d3+856eHiUmJgYdkx4RAABswgrzGK329nbV1NRoYGBAAwMDOnDggKZNm+a/n5ycrISEBLW0tEiS9u7dq6ysrKBjUhEBAMAmxnqL9+zsbB09elS5ubm66qqrNHfuXOXk5Ki4uFgVFRVKS0vTpk2bVFNTo97eXqWmpqqwsDDomA7Litz2J3HjkyM1FYCP0awKmBN/7dSIzvc/SQVhPff1d395hSMZPSoiAADYhM90AGEgEQEAwCYs8a0ZAABgiC8KvzVDIgIAgE34qIgAAABTonFphn1EAACAMVREAACwCd6aAQAAxkTj0gyJCAAANkFFBAAAGEMiAgAAjGFpBgAAGOOLvjyERAQAALtgQzMAAGBMFO7wTiICAIBd0KwKAACM8TlYmgEAAIawNAMAAIxhaQYAABjD67sAAMAYXt8FAADG0CMCAACMicalmXGmAwAAALGLiggAADbBWzMAAMAYekQAAIAxY90j4na7tW/fPklSdna2Vq5cGXB/9+7dcjqdkqTFixdr6dKlQcckEQEAwCbGcmmmublZr776qhoaGuRwOPSDH/xAL730kubMmeP/TWtrqzZv3qyMjIxRj0siAgCATYSbiHi9Xnm93oDrTqfTX91wuVxavXq1xo8fL0m6+eab1dHRMez3ra2t2rZtmzwej2bMmKFVq1YpISEh6Ny8NQMAgE1YjvCO2tpazZ49O+Cora31j33LLbcoPT1dknTq1Cnt27dP2dnZ/vt9fX1KSUlRVVWVGhoa5PV6tXXr1pAxOyzLilhvS9z45EhNBeBj5zteMR0CELPir50a0fm2/v13w3ruu29sDVkR+Yu33npLK1asUHl5ufLy8i455vHjx1VdXa3Gxsagc7M0AwCATYS7NDNSwjGSlpYWVVRUqLq6Wjk5OcPudXR0qLm5WQUFBZIky7IUFxc6zWBpBgAAm7DCPEbjzJkzuu+++7Rp06aAJESSJkyYoI0bN+r06dOyLEt1dXXDGlkvhYoIAAA2MZav7z711FP66KOP9Mgjj/iv3XPPPXr55ZdVUVGhtLQ0rV+/XqWlpRocHNTtt9+uZcuWhRyXHhHA5ugRAcyJdI/IlhvC6xGp/OOzVziS0aMiAgCATbDFOwAAMIYt3gEAgDFjvcX7WCARAQDAJliaAQAAxrA0AwAAjPFFYSrChmYAAMAYKiIAANgEPSIAAMCY6FuYIREBAMA2qIgAAABj2EcEAAAYE41vzZCIAABgE9GXhpCIAABgG/SIAAAAY1iaAQAAxkRfGkIiAgCAbbA0AwAAjGFpBgAAGBN9aQiJCAAAtsHSDAAAMMaKwpoIiQgAADZBRQQAABgTjc2q40wHAAAAYhcVEYzK00/9XK2tbdq8ZZvpUICYsfGJJ9X021f0N5//vCTpphum6N9++q+Go8JnWfTVQ0hEEMKtt35JTzz2kGbOzFBra5vpcICY8r/HjmvjA6uVkfaPpkNBlBjrpRm32619+/ZJkrKzs7Vy5cph99va2rRmzRr19fVp+vTpeuCBBxQXFzzVYGkGQZWWfE9PbX9Ov9z936ZDAWLKwMCA2t46qe11v1TevaX6UfUGnXm3y3RY+IzzhXmMRnNzs1599VU1NDSosbFRb7zxhl566aVhv6mqqtLatWvV1NQky7JUX18fclwSEQT1wx/V6PnnG02HAcScrp73dcftX1X5vxRqz46tui31VpWvfkCWFY3Fd0SKFeYfr9er9vb2gMPr9frHdrlcWr16tcaPH6/4+HjdfPPN6ujo8N/3eDzq7+9Xenq6JCk/P1/79+8PGXPQesknJxjJ9ddfH3ICAMDlm3J9kv7j337qP1+25Nva9l/PyXOmU1OuTzIYGT7Lwn19t7a2Vm63O+B6WVmZysvLJUm33HKL//qpU6e0b98+7dy503+tq6tLLpfLf+5yudTZ2Rly7qCJyIoVK3Tq1CklJiYGZOEOh0MHDhwIOQEA4PKdePv/dOLtd/St+bP91yxLiou7ymBU+KwLd0OzoqIi5eXlBVx3Op0B19566y2tWLFCK1eu1E033eS/7vP55HA4/hqLZQ07v5SgicjOnTu1ZMkSrVu3TtOmTQs5GADgyhg3zqFHfv6fuv22VE25Pkm/aPi1vvylLyop0RX6YcSscCsiTqdzxKTjYi0tLaqoqFB1dbVycnKG3UtKSlJ3d7f/vKenR4mJiSHHDJqIfO5zn9OGDRu0a9cuEhEAiKBbpt6kf60sVdnKn+iCz6fJrmu18SerTIeFzzjfGPYQnTlzRvfdd5+2bNmizMzMgPvJyclKSEhQS0uLpk2bpr179yorKyvkuA4rgp1PceOTIzUVgI+d73jFdAhAzIq/dmpE5/vujflhPffsH/aE/M2GDRu0e/du3XDDDf5r99xzj15++WVVVFQoLS1Nb775pmpqatTb26vU1FQ9/PDDGj9+fNBxSUQAmyMRAcyJdCKy5MbAPo/ReO4PDVc4ktFjQzMAAGyCr+8CAABj+PouAAAwJhq/vksiAgCATbA0AwAAjGFpBgAAGBON3yLio3cAAMAYKiIAANgEzaoAAMAYekQAAIAxvDUDAACMYWkGAAAYE41vzZCIAABgE/SIAAAAY+gRAQAAxtAjAgAAjKFHBAAAGENFBAAAGEOPCAAAMMbH0gwAADAl+tIQEhEAAGyDHhEAAGAMiQgAADAmGl/fHWc6AAAAELuoiAAAYBPRuDRDRQQAAJuwwvxzOXp7e7Vw4UK1t7cH3HO73brrrru0aNEiLVq0SHV1dSHHoyICAIBNjHWPyJEjR1RTU6NTp06NeL+1tVWbN29WRkbGqMckEQEAwCbCXZrxer3yer0B151Op5xOp/+8vr5e69at08qVK0ccp7W1Vdu2bZPH49GMGTO0atUqJSQkBJ2bRAQAAJsItyJSW1srt9sdcL2srEzl5eX+8wcffPCSY/T19SklJUVVVVW68cYbtXr1am3dulWVlZVB5yYRAQDAJsKtiBQVFSkvLy/g+ierIaFcffXVevLJJ/3ny5cvV3V1NYkIAACxItyP3l28BBOOjo4ONTc3q6Cg4M+xWJbi4kKnGbw1AwCATfgsK6zjSpgwYYI2btyo06dPy7Is1dXVac6cOSGfIxEBAMAmIvH67sWKi4t17NgxXXPNNVq/fr1KS0s1f/58WZalZcuWhXzeYUVwP9i48cmRmgrAx853vGI6BCBmxV87NaLzpSTODOu5tq7DVziS0aNHBAAAm/i01Q0TSEQAALCJK9XvEUkkIgAA2AQVEQAAYAwVEQAAYAwVEQAAYIxl+UyHcNnYRwQAABhDRQQAAJsI91szJpGIAABgExHco/SKIREBAMAmqIgAAABjqIgAAABj2EcEAAAYwz4iAADAGJZmAACAMTSrAgAAY6iIAAAAY2hWBQAAxlARAQAAxtAjAgAAjKEiAgAAjKFHBAAAGMOGZgAAwBgqIgAAwJho7BEZZzoAAAAQu0hEAACwCSvMP5ejt7dXCxcuVHt7e8C9trY25efna968eVqzZo2GhoZCjkciAgCATViWFdYxWkeOHNF3vvMdnTp1asT7VVVVWrt2rZqammRZlurr60OOSSICAIBNhJuIeL1etbe3Bxxer3fY+PX19Vq3bp0SExMD5vZ4POrv71d6erokKT8/X/v37w8Zc0SbVYcGPJGcDgCAmDIY5n9nn3jiCbnd7oDrZWVlKi8v958/+OCDlxyjq6tLLpfLf+5yudTZ2Rlybt6aAQAgxhUVFSkvLy/gutPpHPUYPp9PDofDf25Z1rDzSyERAQAgxjmdzstKOkaSlJSk7u5u/3lPT8+ISzgXo0cEAAB8asnJyUpISFBLS4skae/evcrKygr5HIkIAAAIW3FxsY4dOyZJ2rRpkx5++GHNnz9f586dU2FhYcjnHVY0bsMGAABsgYoIAAAwhkQEAAAYQyICAACMIREBAADGkIggpF/96ldasGCB5s6dq7q6OtPhADEl2AfGADsgEUFQnZ2d2rJli5577jk1NjbqF7/4hd5++23TYQExIdQHxgA7IBFBUM3NzZo1a5a+8IUvaNKkSZo3b96oPmIE4NML9oExwC7Y4h1BXfwRo8TERB09etRgREDsCPaBMcAuqIggqHA/YgQAwGiQiCCoiz9i1N3dTZkYAHDFkIggqK997Ws6ePCg3n//fZ0/f16/+c1vRvURIwAARoMeEQQ1efJkVVZWqrCwUIODgyooKNBtt91mOiwAgE3w0TsAAGAMSzMAAMAYEhEAAGAMiQgAADCGRAQAABhDIgIAAIwhEQEAAMaQiAAAAGNIRAAAgDH/D4I8f7YTCM2IAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#From the results it can be observed that there is only one misclassification for ICU cases and 1 misclassification for NonICU cases  which means our model is a good prediction model.\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictions\n",
    "\n",
    "\n",
    "27. The resutls below show the actual values against the fitted(test data) ones and we see a high level of accuracy which menas our model is able to predict unseen data accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>fitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual  fitted\n",
       "82       0       0\n",
       "17       1       1\n",
       "97       0       0\n",
       "31       0       0\n",
       "45       0       0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's now see how our model peformed in train and test data.\n",
    "\n",
    "#This block is associated with train and test data respectively.\n",
    "#it shows the result of the  actual values against fitten ones.\n",
    "lreg_train = pd.DataFrame({ \"actual\": y_train, \"fitted\": pipe.predict(X_train) })\n",
    "lreg_test = pd.DataFrame({ \"actual\": y_test, \"fitted\": pipe.predict(X_test) })\n",
    "\n",
    "\n",
    "rndf_train = pd.DataFrame({ \"actual\": y, \"fitted\": rndf_clf.predict(X) })\n",
    "\n",
    "lreg_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>fitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual  fitted\n",
       "9         0       0\n",
       "72        0       0\n",
       "12        1       1\n",
       "107       0       0\n",
       "37        0       0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This shows the tail result of the actual values against the fitted ones\n",
    "lreg_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discussion\n",
    "\n",
    "\n",
    "Let's define a pipeline as follow:\n",
    "\n",
    "Standardization was done for X for because the features of the iput data X have difference beteween their range(some columns have values as low as 0.1 and others as high as 430.35). Standardization was used to transform features to comparable scales before implementing SVM\n",
    "\n",
    "SVM was one models used first to compare accuracries accross models before picking the best fit for the data. I decided to try linear SVM kernel because the data had a large number of features and it is linearly separable in high dimensional space.Accuracy of all SVM kernel of the test data set was printed out for evaluation. I did not show to prediction result and confusion matrix for SVM because i ended up selecting logistic regression.\n",
    "\n",
    "Creating tuning and validation data for k-cross validation before using Logistic regression. This was done to ensure that every observation in the dataset has chance to appearing because performs training 'k'number of time which gives a better insight.Tuning data is composed by 80% of data, the remaining 20% of data goes to the validation set.The variable random_state indicates the seed of the shuffling for reproducibility reasons\n",
    "\n",
    "LOGISTIC REGRESSION\n",
    "As for the logistic regression, we are using the LogisticRegression module from sklearn.linear_model.\n",
    "\n",
    "Standardisation of features:- Standardization was used to transform features to comparable scales before implementing logistic regression\n",
    "Logistic regression was used since we were solving a binary classification problem.\n",
    "Random Forest was also explored to further see the best model because it is a model good for datasets with numerous features\n",
    "\n",
    "Some important parameters defined in the LogisticRegression module are:\n",
    "\n",
    "C. Inverse of regularization strength; must be a positive float.Smaller values specify stronger regularization which is why i used 1.\n",
    "Random_state was used reproducibility reasons.\n",
    "\n",
    "From the results it can be observed that there is only 1 misclassification for ICU cases and 1 misclassification for NonICU cases with Logistic regression and the accuracy of training to test set was excellent\n",
    "\n",
    "Also the prediction result(actual and fitted) on unseen data shows linear regression performs well on the unseen data.\n",
    "\n",
    "The limitation of my chosen model is that it has a very high probability of ignoring outliers in the data and it is not always the best model for a dataset with way more features than observations.\n",
    "\n",
    "ALternatively, SVM is a second option to solve the given machine learning question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It took my chosen machne learning model 2.262s to run\n",
    "Logistic Regression\n",
    "\n",
    "Train Time Complexity=O(n*m)\n",
    "Test Time Complexity=O(m)\n",
    "Space Complexity = O(m)\n",
    "n = number of training examples, m = number of features\n",
    "\n",
    "\n",
    "Overall time complexity = T(n) = n - 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
